{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "from xgboost import XGBClassifier\n",
    "from warnings import simplefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_sentiment(tweet):    \n",
    "    analysis = TextBlob((str(tweet)))     #defining the function which will find the plority of a sentence\n",
    "    return analysis.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TABLE OF FREQUENCY WORD DISTRIBUTION (692, 4514)\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('Train_Data.csv')\n",
    "# SPLITTING THE DATASET INTO TRAINING AND TESTING\n",
    "train = df[df['Date'] < '2011-05-09']\n",
    "test = df[df['Date'] > '2011-05-18']\n",
    "\n",
    "# CONVERT THE TRAINNG DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "train_list = []\n",
    "for row in range(0,len(train.index)): \n",
    "    train_list.append(' '.join(str(k) for k in train.iloc[row,2:27]))\n",
    "    \n",
    "# DEFINING THE VECTOR FUNCTION, SPECIFYING THR MIN AND MAX WORD FREQUENCY FILTER  \n",
    "vectorize= CountVectorizer(min_df=0.01, max_df=0.8)\n",
    "\n",
    "# TRANSFORMING THE TRAINING DATASET INTO WORD FREQUENCY TRANFORMATION\n",
    "train_vector = vectorize.fit_transform(train_list) \n",
    "print( \"THE TABLE OF FREQUENCY WORD DISTRIBUTION\" , train_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the baseline model accuracy 0.5144508670520231\n",
      "Top ten words according to the baseline model             Word  Coefficient\n",
      "3031   political     0.563931\n",
      "400   australian     0.494684\n",
      "1690      forced     0.462982\n",
      "2396        life     0.452875\n",
      "2218      jewish     0.436600\n",
      "4341      wanted     0.434160\n",
      "1985       homes     0.426856\n",
      "1545       faces     0.425054\n",
      "3708          so     0.422071\n",
      "3805       state     0.414759\n",
      "Last ten words according to the baseline model           Word  Coefficient\n",
      "4274  vehicles    -0.373066\n",
      "2197     italy    -0.391548\n",
      "3544    search    -0.391845\n",
      "65          40    -0.393927\n",
      "2015       how    -0.442358\n",
      "517        big    -0.472416\n",
      "692     canada    -0.489905\n",
      "2670        my    -0.491770\n",
      "2483      many    -0.493135\n",
      "2698      need    -0.513944\n",
      " TFID TRANSFOMATION DATAFRAME SHAPE (692, 266)\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression()\n",
    "\n",
    "model = lr.fit(train_vector, train[\"Label\"])\n",
    "\n",
    "test_list = []\n",
    "\n",
    "# CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "for row in range(0,len(test.index)):\n",
    "    test_list.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "  \n",
    "# TRANSFORMING THE TESTING DATASET INTO WORD FREQUENCY TRANFORMATION\n",
    "test_vector = vectorize.transform(test_list) \n",
    "\n",
    "predictions = model.predict(test_vector)\n",
    "\n",
    "pd.crosstab(test[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy1=accuracy_score(test['Label'], predictions)\n",
    "\n",
    "print(\"the baseline model accuracy\", accuracy1)\n",
    "\n",
    "# WORD DISTRIBUTION OF THE MODEL\n",
    "words = vectorize.get_feature_names()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : words,'Coefficient' : coefficients})  \n",
    "\n",
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "print(\"Top ten words according to the baseline model\",coeffdf.head(10))\n",
    "print(\"Last ten words according to the baseline model\",coeffdf.tail(10))\n",
    "\n",
    "\n",
    "# DEFINING THE TFID TRANSFORMATION FUNCTION\n",
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.85,ngram_range=(2,2)) \n",
    "news_nvector = nvectorize.fit_transform(train_list)\n",
    "\n",
    "print(\" TFID TRANSFOMATION DATAFRAME SHAPE\",news_nvector.shape)\n",
    "\n",
    "nmodel = lr.fit(news_nvector, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistics Regression with Bigram and TFID 0.5361271676300579\n",
      "Random forest with tfid and bigram 0.5\n"
     ]
    }
   ],
   "source": [
    "test_list = []\n",
    "# CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "for row in range(0,len(test.index)):\n",
    "    test_list.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    " \n",
    "ntest_vector = nvectorize.transform(test_list)\n",
    "npredictions = nmodel.predict(ntest_vector)\n",
    "\n",
    "pd.crosstab(test[\"Label\"], npredictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy2=accuracy_score(test['Label'], npredictions)\n",
    "print(\"Logistics Regression with Bigram and TFID\",accuracy2)\n",
    "\n",
    "nwords = nvectorize.get_feature_names()\n",
    "ncoefficients = nmodel.coef_.tolist()[0]\n",
    "ncoeffdf = pd.DataFrame({'Word' : nwords,'Coefficient' : ncoefficients})\n",
    "ncoeffdf = ncoeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "ncoeffdf.head(10)\n",
    "ncoeffdf.tail(10)\n",
    "\n",
    "\n",
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.95,ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_list)\n",
    "\n",
    "#DEFINNG THE RANDOM FOREST MODEL\n",
    "rfmodel = RandomForestClassifier(random_state=55)  \n",
    "rfmodel = rfmodel.fit(news_nvector, train[\"Label\"])\n",
    "test_list = []\n",
    "for row in range(0,len(test.index)):\n",
    "    test_list.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "ntest_vector = nvectorize.transform(test_list)\n",
    "\n",
    "rfpredictions = rfmodel.predict(ntest_vector)\n",
    "accuracyrf = accuracy_score(test['Label'], rfpredictions)\n",
    "print(\"Random forest with tfid and bigram\", accuracyrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy:  0.5433526011560693\n",
      "gbpredictions 17.0700274965675\n",
      " CONFUSION MATRIX OF THE GRADIANT BOOSTING  [[ 68 247]\n",
      " [ 95 282]]\n",
      "Gradient Boosting accuracy:  0.5057803468208093\n",
      "(692, 251230)\n",
      "TRIGARAM ACCURACY 0.5447976878612717\n",
      "15.722494852004878\n",
      "trigram top ten word distibution                           Word  Coefficient\n",
      "244050             will not be     0.132308\n",
      "101163             in favor of     0.121931\n",
      "112457          israel and the     0.117012\n",
      "97394       human rights watch     0.116544\n",
      "141374       nobel peace prize     0.104012\n",
      "139891          new york times     0.103986\n",
      "78739   founder julian assange     0.100721\n",
      "24006              at least 10     0.097888\n",
      "149833             of the most     0.097688\n",
      "89831           has been found     0.097416\n",
      "trigram last ten word distibution                     Word  Coefficient\n",
      "205544   the conflict in    -0.119483\n",
      "219430       to death in    -0.128549\n",
      "102930       in order to    -0.132749\n",
      "222023        to pay for    -0.136040\n",
      "25447       aung san suu    -0.137861\n",
      "182423       san suu kyi    -0.137861\n",
      "65705   embassy in yemen    -0.139741\n",
      "162328  people have been    -0.141636\n",
      "247931      world war ii    -0.155400\n",
      "156703   osama bin laden    -0.208338\n"
     ]
    }
   ],
   "source": [
    "# DEFINING THE NAIVE BAYS MODEL\n",
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.8,ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_list)\n",
    "\n",
    "nbmodel = MultinomialNB(alpha=0.5)\n",
    "nbmodel = nbmodel.fit(news_nvector, train[\"Label\"])\n",
    "\n",
    "# CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "test_list = []\n",
    "for row in range(0,len(test.index)):\n",
    "    test_list.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "    \n",
    "ntest_vector = nvectorize.transform(test_list)\n",
    "\n",
    "nbpredictions = nbmodel.predict(ntest_vector)\n",
    "nbaccuracy=accuracy_score(test['Label'], nbpredictions)\n",
    "\n",
    "print(\"Naive Bayes accuracy: \",nbaccuracy)\n",
    "\n",
    "#author: Shravan Chintha\n",
    "#Gradient Boosting Classifier\n",
    "# DEFINING THE GARDIANT BOOSTING MODEL\n",
    "gbmodel = GradientBoostingClassifier(random_state=52) \n",
    "gbmodel = gbmodel.fit(news_nvector, train[\"Label\"])\n",
    "test_list = []\n",
    "for row in range(0,len(test.index)):\n",
    "    test_list.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "ntest_vector = nvectorize.transform(test_list)\n",
    "\n",
    "gbpredictions = gbmodel.predict(ntest_vector.toarray())\n",
    "gbaccuracy = accuracy_score(test['Label'], gbpredictions)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "logloss = log_loss(test['Label'], gbpredictions, eps=1e-15)\n",
    "print(\"gbpredictions\" ,logloss)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\" CONFUSION MATRIX OF THE GRADIANT BOOSTING \", confusion_matrix(test['Label'], gbpredictions))\n",
    "\n",
    "print(\"Gradient Boosting accuracy: \",gbaccuracy)\n",
    "\n",
    "\n",
    "# DEFINING THE TFID , TRIGRAM MODEL\n",
    "n3vectorize = TfidfVectorizer(min_df=0.0004, max_df=0.115,ngram_range=(3,3)) \n",
    "news_n3vector = n3vectorize.fit_transform(train_list)\n",
    "\n",
    "print(news_n3vector.shape)\n",
    "\n",
    "n3model = lr.fit(news_n3vector, train[\"Label\"])\n",
    "\n",
    "# CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "test_list = []\n",
    "for row in range(0,len(test.index)):\n",
    "    test_list.append(' '.join(str(x) for x in test.iloc[row,2:27])) \n",
    "    \n",
    "n3test_vector = n3vectorize.transform(test_list)\n",
    "n3predictions = n3model.predict(n3test_vector)\n",
    "\n",
    "pd.crosstab(test[\"Label\"], n3predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy3=accuracy_score(test['Label'], n3predictions)\n",
    "\n",
    "print(\"TRIGARAM ACCURACY\", accuracy3)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "logloss = log_loss(test['Label'], n3predictions, eps=1e-15)\n",
    "print(logloss)\n",
    "\n",
    " # trigram model word distribution\n",
    "n3words = n3vectorize.get_feature_names()\n",
    "n3coefficients = n3model.coef_.tolist()[0]\n",
    "n3coeffdf = pd.DataFrame({'Word' : n3words,'Coefficient' : n3coefficients})\n",
    "n3coeffdf = n3coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "print(\"trigram top ten word distibution\", n3coeffdf.head(10))\n",
    "print(\"trigram last ten word distibution\", n3coeffdf.tail(10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 96 219]\n",
      " [109 268]]\n",
      "Sentiment Accuracy 0.5260115606936416\n",
      "f1_score__ 0.506051043262026\n",
      "16.371233770762885\n"
     ]
    }
   ],
   "source": [
    "train_sentiment=train\n",
    "test_sentiment = test\n",
    "train_sentiment =train_sentiment.drop(['Date', 'Label','key'], axis=1)\n",
    "for column in train_sentiment:\n",
    "    #converting the train headlines into polarity scores\n",
    "    train_sentiment[column]=train_sentiment[column].apply(analize_sentiment)  \n",
    "    # removing negative co:efficient from the datset for better performance\n",
    "    train_sentiment = train_sentiment\n",
    "\n",
    "\n",
    "test_sentiment =test_sentiment.drop(['Date', 'Label','key'], axis=1)\n",
    "for column in test_sentiment:\n",
    "    test_sentiment[column]=test_sentiment[column].apply(analize_sentiment) \n",
    "    test_sentiment=test_sentiment \n",
    "# training the polarity score datset with DIJA \n",
    "XGB_model= XGBClassifier()  \n",
    "gradiant=XGB_model.fit(train_sentiment, train['Label'])\n",
    "y_pred= gradiant.predict(test_sentiment)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test['Label'], y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Sentiment Accuracy\",accuracy_score(test['Label'], y_pred))\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"f1_score__\",f1_score(test['Label'], y_pred, average='weighted'))\n",
    "from sklearn.metrics import log_loss\n",
    "logloss = log_loss(test['Label'], y_pred, eps=1e-15)\n",
    "print(logloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
